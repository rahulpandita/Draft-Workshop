---
layout: post
title:  "log.info(“What are the developers logging?”);"
date:   2015-09-22 16:00:00
categories: Review
---

*Software logging is a pervasive practice in large production systems. Besides being beneﬁcial in accounting for the activity within production system, logging is extremely valuable to keep track of what went wrong in case of a failure. However, recent studies point out the arbitrary nature of the logging within current systems. Absence of formal rigorous speciﬁcation and systematic process adds to the arbitrary nature of exiting logging practices. Since logging is seldom a core functionality intended towards the users of software system, therefore, in many cases, log messages are written as ‘after-thoughts’ after a failure happens and logs are needed. We hypothesize that existing logging patterns in the open source repositories is a source of collective knowledge a lot of developers. Thus, these patterns can assist developers in making informed logging decisions. In this paper, we present a novel approach based on mining data from exiting software repositories to mine logging patterns. Our evaluation results show ...*

####INTRODUCTION

Software logging is a pervasive practice in production systems. The importance of logging can be traced to as early as Kernighan and Pikes The Practice of Programming. Besides being useful to keep track of program execution and auditing, software logging provides invaluable information to debug a software system in event of a failure. Logged messages depending upon the severity level can be used to gain insights into the last running state (last guaranteed executed instruction) of the system before the event of failure. Thus, assisting a developer to begin the debugging process. With additional, information such as logged values of the variables, stack-traces provides further insights into the state of the system before the failure event. Thus, logging serves a indispensable tool to assist developers in debugging a failure in the system. Thus, practically almost all opensource software print log messages. For example, the widely used OpenSSH server contains 3407 log printing statements in its 81K lines of code. Owing to the usefulness of logging a number of logging frameworks are available for different programming languages.

Typically, a logging framework is broken into three major components: the Logger, Formatter and the Handler (Appender). The Logger is responsible for capturing the message to be logged along with certain meta-data and passing it to the formatter. The Formatter formats it for output. The framework then forwards the formatted message to the appropriate Appender for disposition. This might include a console display, writing to disk, appending to a database, or email.

A logging message usually consists of a natural language text describing the event that is being logged, sometimes accompanied with active variables names and values at the location of the logging statement. Furthermore, a logging message is usually logged under a certain severity level. Apache commons logging [1] recommends following six levels of severity :

1. Fatal - This level is associated with severe errors that cause premature termination of the software.
2. Error - This level is associated with runtime errors or unexpected conditions.
3. Warn - This level is associated with ‘almost’ errors and other runtime situations that are undesirable or unexpected, but not necessarily ‘wrong’.
4. Info - This level is associated with interesting runtime events (startup/shutdown).
5. Debug - This level is associated with detailed information on execution of the system.
6. Trace - This level is associated with more detailed information than ‘debug’ level.

Despite being highly useful and recent studies [2] have shown logging to be subjective and arbitrary in practice. Although there exists guidelines such as the one from Apache commons logging (described above), absence of formal rigorous speciﬁcation and systematic process adds to the arbitrary nature of exiting logging practices. Recently, a few research work proposed to systematically improve a few aspects of software logging. LogEnhancer [3] automatically suggests which variable values need to be recorded additionally in each existing log message; LogErr [4] proposes to systematically log common exceptions such as system call return errors. However, even with these tools, given the subjectiveness of logging developers still need to make a lot of logging decisions.

As [2] points out “logging is seldom a core functionality intended towards the users of software system, therefore, in many cases, log messages are written as ‘after-thoughts’ after a failure happens and logs are needed”. Based upon the insight, we hypothesize that existing logging patterns in the open source repositories is a source of collective knowledge a lot of developers. Thus, these patterns can assist developers in making informed logging decisions.

To assist developers in making logging decisions, we present a novel approach based on mining data from exiting software repositories. Mining based approaches have been shown to quite useful in software testing and veriﬁcation [5], [6] earlier. We adapt these techniques for assisting developers with making logging decisions. In particular, we employ data mining techniques on exiting software repositories to dynamically mine logging patterns.

We have implemented a prototype of the proposed approach to assist Java developers in making logging decisions with Log4J framework. However, our approach is not speciﬁc to any framework and can be used independent of programming languages as well. Our approach makes the following contributions:

1. An approach to assist developers in making logging decisions.
2. Prototypical implementation of our approach to collect logging patterns dynamically from the open source repositories.
3. A technique to analyze partial code samples from the open source repositories based on Abstract Syntax Trees (AST’s) capable of approximate data-ﬂow analysis and control-ﬂow analysis.

The rest of the paper is organized as follows. Section II presents the background on code contracts as well as NLP. Section III presents an two real world examples that motivate our approach. Section IV presents our approach. Section V presents evaluation of our approach. Section VI discusses related work. Finally, Section VII concludes.

####CASE STUDY

We now present a case study we conducted to ﬁnd what elements are logged in a Object Oriented Software. We used following open source projects as our subjects for conducting the case study:

1. **Openﬁre**: Openﬁre [7] is a real time collaboration server licensed under the Open Source Apache License. based on EXtensible Message and Presence Protocol (XMPP also called Jabber). Openﬁre is completely written in Java and is immensely popular based on 4,378,567 downloads since its release. Openﬁre uses Simple Logging Facade for Java (SLF4J) [8] framework for logging. Openﬁre has 81.5K lines of code, thus is not a trivial subject.
2. more to come


| Subject | Logger | LOC | Log Stmts | Verbosity Levels |
| ------- | ------ | --- | --------- | ---------------- |
| OpenFire | SLF4J | 81.5K | 1159 | 5 |
| Total    |       |       |      |   | 

Table I : SUBJECT METRICS

#####A. Study Methodology

We studied various aspects of logging in object oriented software. We were intersected in studying logging practices speciﬁc to an API being used. In particular, we summarize our observations in Table II

| Finding | Implication |
| ------- | ----------- |
| **1. Parameter logging** |    |
| 13% Logging statements logged parameters | Values of parameters of a method are sometimes logged to assist in debugging |
| Only 4.5% explicitly log just parameters | Parameter values afe often accompanied with other information such as return values of methods or exception information |
| **2. Return Values** |    |
| 53% Logging statements logged return values of some methods | A significant number of log statements log return values of method calls. |
| **3. Exceptions** |    |
| 41.5% Logging statements logged exceptions | A significant number of statements log exceptions. |
| 15% logging statements logged exceptions with return values | Oftentimes exception logs are accompanied with return values of some methodcalls |
| For every logged exception within a project there are 3 exceptions that are logged from external API library | Exceptions from external API library is logged more often to debug issues that manifest at the interface with those libraries |
| Roughly 50% of all logged exceptions were from `java.sql` API Refer Figure 1 | some API librairies are more significant with respect to logging in comparison to others |
| Exceptions from certain method invocations are always logged | There exist certain methods in an API libraries which are critical, thus requiring explicit logging. For instance `java.util.concurrent.BlockingQueue: Object take()` exceptions are logged every time this method is invoked |
| There exist methods in API libraries whose exceptions are sparingly logged | Not all exceptions from an external method call requires logging. For instance `java.sql.Connection: PreparedStatement prepareStatement(String)` out of 211 invocations 171(81%) times exceptions are logged. |
| There exist methods in API libraries whose exceptions are almost never logged | There exist certain methods in an API libraries which are not critical, thus not requiring explicit logging. For instance `javax.naming.Context: void close()` where exception is never logged out of all 39 invocations |
| **4. Other Logging** |   |
| 13.5% of logging statements cannot be classifed in any of the above cases | There are instances when a log statement is inserted purely for auditing purposes. For instance when entering into a method. Also, there are cases of over cautious logging. For instance logging within a try-catch block to catch runtime exceptions(Exceptions that are not explicitly thrown).|

Table II OBSERVATIONS

We further evaluated the logging related to exception’s. For every logging statement corresponding to explicit exceptions we tried to ﬁnd out what API is being invoked. Figure 1 shows the distribution of API’s whose methods are explicitly being logged. Y-Axis denotes the API whose exceptions are being explicitly being logged and X-Axis denotes the count of number of such method calls. Note, multiple method invocations can be logged by the same logging statement. It is worthwhile to notice that exceptions related to java.sql API are one of the highly logged exceptions.

![Figure 1]({{ baseurl }}/blog/images/apiLogStat.png)


####EXAMPLE

TBD

####APPROACH

![Approach Overview]({{ baseurl }}/blog/images/approachLog.png)

Figure 2 shows the high-level overview of our approach. There are two main components of our approach namely “Code Downloader” and “Code Analyser”. The code downloader accepts an API method call and queries a Code Search Engine such as Koders [9] for relevant source code ﬁles. The code downloader further downloads the relevant source code ﬁles to be analyzed locally. The code analyser component accepts the downloaded source code ﬁles to generate logging rules speciﬁc to the API provided as the input. We now explain each component in detail using illustrative examples.

#####A. Code Search Engine

With an exponential increase in the open-source projects available on web, it is prohibitively resource intensive to have a repository for all the available projects. In contrast there are existing repositories on the web which we can search using a Code Search Engine (CSE). A CSE facilitates searching of relevant code samples from the web using simple queries. We leverage the API’s exposed by a search engine to search for relevant source code. Furthermore, using CSE’s to collect code samples on demand further reduces the bias in the mined logging rules and makes our approach independent of any speciﬁc framework or libraries.

In particular, we use Koders [9], for collecting relevant code samples partly because after Google Code Search Engine1 was shutdown on January 15, 2012, Koders became the largest CSE. However, our approach is independent of any CSE and can work with any exiting CSE’s.

#####B. Code Downloader

Our Code downloader component accepts the queries in the form of API method being invoked by a programmer and constructs a request to CSE. In particular, the constructed request contains invoked methods parent class’s type and a list of available logging API’s such as SLF4J [8] and Log4J [1]. The CSE then returns a list of code snippets that use these logging API’s along with the queried method invocation. The returned results are then downloaded to be analyzed locally. The downloaded code samples are not complete as only individual source ﬁles using the API’s are downloaded instead of entire projects.

#####C. Code Analyzer

Code Analyzer component accepts the code snippets downloaded by the code downloader component and analyzes them to mine for logging rules. Since, the code snippets are often not compilable, we use partial static analysis techniques as used by Thummalapenta et. al. in [6]. In particular, Code Analyzer component ﬁrst constructs a Abstract Syntax Tree (AST) from the source code ﬁles. We then use the constructed AST’s to generate a Control Flow Graph (CFG) not considering the exception paths. Each node in CFG is a basic block. A basic block is a list of statements that are executed without a jump. An edge ‘ab →’ from node ‘a’ to ‘b’ means that a basic block representing node ‘b’ is executed after execution of basic block representing node ‘a’.

It is worthwhile to notice that constructed CFG does not account for exception behavior, thus is not suitable for mining logging patterns related to exception handling. Thus we further construct approximate Exception Flow Graphs (EFG’s), which are an extended form of Control-Flow Graphs (CFG). An EFG provides a graphical representation of all paths that might be traversed during the execution of a program, including exception paths.

Construction of EFG is not trivial as exception ﬂow involves transfer of control from a statement to a catch-block or to the throws construct in the method declaration. Since the code snippets are often not compilable, it is often not known before hand the explicit exceptions thrown by method call invocations.

We use the following algorithm to construct an approximate EFG. We demonstrate that the approximate EFG serves our purpose of mining logging patterns. To the best of our knowledge there are no existing techniques to perform such analysis. Once CFG and EFG are created, we extract the method invocation and corresponding logging statement pair as follows. First we identify the nodes (Ns) in the CFG corresponding to the API method invocation provided as the input by the user. We next identify the nodes (Nd) in the CFG that are logging statements corresponding to Nd.

We perform simple data-ﬂow analysis to determine if Nd correspond to Ns. We consider a Nd to be corresponding to a Ns if any one of the following conditions are true:

1. the logging statement in Nd logs return value of method call in Ns
2. the logging statement in Nd lies in a ‘if then else’ structure where decision directly based on return value of method call in Ns
3. Nd is the only and immediate descendant of Ns We further analyses EFG’s and extract the method invocation and corresponding logging statement pair as follows.

We identify the the statement (Ss) corresponding to method invocation provides as the input by the user. We the traverse the EFG’s to ﬁnd if there exists an exception path between the Ss to a logging statement (Sd). If such a path exist’s we extract the Ss and Sd as the logging pair.

**1) Log Clustering:** We next describe the heuristics used by our approach for clustering method invocation and corresponding log statements. To identify similar log statements we ﬁrst omit the static string messages from the log statements and only consider the involved variables.

This heuristics is based on the observation that oftentimes, the static message in a logging statement is application dependent. However, developer do log the critical variables values to assist in debugging process. We further group together the log messages by log levels. We group the logging statements by the levels of severity. In particular, we divide the log statement into three groups:

1) Fatal/Error, 2)Warn/Info, and 3) Debug/Trace.

This heuristic is based on the observation that developers oftentimes use levels of severity interchangeably along these dimensions due to no formal distinction between levels.

**2) Result Presentation:** The resultant clusters from the previously described “Log Clustering” section are then presented to the user in the decreasing order of their sizes.

#####D. Implementation

We use Koders [9] as the CSE for collecting relevant code samples. We use Jsoup to interact with Koders. Jsoup2 is a Java library for working with HTML providing a rich API for extracting and manipulating HTML data. We then use JavaParser3 an open source Java library to generate AST’s from downloaded source ﬁles. JavaParser is Java 1.5 Parser with AST generation and visitor support. We then implemented our partial code analysis in Java programming language. Our tool is freely available on the project website.

####REFERENCES

1. “Apache Commons Logging,” http://commons.apache.org/ logging/.
2. D. Yuan, S. park, and Y. Zhou, “Characterizing logging practices in open-source software,” in Proc. 34th ICSE, 2012.
3. D. Yuan, J. Zheng, S. Park, Y. Zhou, and S. Savage., “Improving software diagnosability via log enhancement,” ACM Trans. Computer Systems (TOCS), 2012.
4. D. Yuan, S. Park, P. Huang, Y. Liu, M. Lee, X. Tang, Y. Zhou, and S. Savage., “Did you log the error?” in Technical Report, 2012.
5. S. Thummalapenta and T. Xie, “Alattin: Mining alternative patterns for detecting neglected conditions,” in Proc. 24th IEEE/ACM International Conference on Automated Software Engineering (ASE 2009), November 2009, pp. 283– 294. [Online]. Available: http://www.csc.ncsu.edu/faculty/xie/ publications/ase09-alattin.pdf
6. S. Thummalapenta and T. Xie, “Parseweb: A programmer assistant for reusing open source code on the web,” in Proc. 22nd ASE, November 2007, pp. 204–213.
7. “Jive Software: OpenFire,” http://www.igniterealtime.org/ projects/openﬁre/.
8. “Simple Logging Facade for Java (SLF4J),” http://www.slf4j. org/.
9. “Black Duck Softwares Koders,” http://www.koders.com/.